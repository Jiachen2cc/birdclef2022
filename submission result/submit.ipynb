{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\vit\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import wget\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "\n",
    "# Audio \n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram, Resample,AmplitudeToDB\n",
    "\n",
    "# Utils\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "from timm.models.layers import to_2tuple,trunc_normal_\n",
    "\n",
    "# For colored terminal text\n",
    "from colorama import Fore, Back, Style\n",
    "b_ = Fore.BLUE\n",
    "sr_ = Style.RESET_ALL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For descriptive error messages\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    num_class = 152\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    #device = torch.device('cpu')\n",
    "    #model_name = 'tf_efficientnet_b0_ns'\n",
    "    model_name = ['AST','tf_efficientnet_b0_ns']\n",
    "    embedding_size = 768\n",
    "    #Audio Specific\n",
    "    sample_rate = 32000\n",
    "    max_time = 5\n",
    "    n_mels = 224\n",
    "    n_fft = 1024\n",
    "    period = 30\n",
    "    \n",
    "    target_columns = [\n",
    "        \"afrsil1\",\n",
    "        \"akekee\",\n",
    "        \"akepa1\",\n",
    "        \"akiapo\",\n",
    "        \"akikik\",\n",
    "        \"amewig\",\n",
    "        \"aniani\",\n",
    "        \"apapan\",\n",
    "        \"arcter\",\n",
    "        \"barpet\",\n",
    "        \"bcnher\",\n",
    "        \"belkin1\",\n",
    "        \"bkbplo\",\n",
    "        \"bknsti\",\n",
    "        \"bkwpet\",\n",
    "        \"blkfra\",\n",
    "        \"blknod\",\n",
    "        \"bongul\",\n",
    "        \"brant\",\n",
    "        \"brnboo\",\n",
    "        \"brnnod\",\n",
    "        \"brnowl\",\n",
    "        \"brtcur\",\n",
    "        \"bubsan\",\n",
    "        \"buffle\",\n",
    "        \"bulpet\",\n",
    "        \"burpar\",\n",
    "        \"buwtea\",\n",
    "        \"cacgoo1\",\n",
    "        \"calqua\",\n",
    "        \"cangoo\",\n",
    "        \"canvas\",\n",
    "        \"caster1\",\n",
    "        \"categr\",\n",
    "        \"chbsan\",\n",
    "        \"chemun\",\n",
    "        \"chukar\",\n",
    "        \"cintea\",\n",
    "        \"comgal1\",\n",
    "        \"commyn\",\n",
    "        \"compea\",\n",
    "        \"comsan\",\n",
    "        \"comwax\",\n",
    "        \"coopet\",\n",
    "        \"crehon\",\n",
    "        \"dunlin\",\n",
    "        \"elepai\",\n",
    "        \"ercfra\",\n",
    "        \"eurwig\",\n",
    "        \"fragul\",\n",
    "        \"gadwal\",\n",
    "        \"gamqua\",\n",
    "        \"glwgul\",\n",
    "        \"gnwtea\",\n",
    "        \"golphe\",\n",
    "        \"grbher3\",\n",
    "        \"grefri\",\n",
    "        \"gresca\",\n",
    "        \"gryfra\",\n",
    "        \"gwfgoo\",\n",
    "        \"hawama\",\n",
    "        \"hawcoo\",\n",
    "        \"hawcre\",\n",
    "        \"hawgoo\",\n",
    "        \"hawhaw\",\n",
    "        \"hawpet1\",\n",
    "        \"hoomer\",\n",
    "        \"houfin\",\n",
    "        \"houspa\",\n",
    "        \"hudgod\",\n",
    "        \"iiwi\",\n",
    "        \"incter1\",\n",
    "        \"jabwar\",\n",
    "        \"japqua\",\n",
    "        \"kalphe\",\n",
    "        \"kauama\",\n",
    "        \"laugul\",\n",
    "        \"layalb\",\n",
    "        \"lcspet\",\n",
    "        \"leasan\",\n",
    "        \"leater1\",\n",
    "        \"lessca\",\n",
    "        \"lesyel\",\n",
    "        \"lobdow\",\n",
    "        \"lotjae\",\n",
    "        \"madpet\",\n",
    "        \"magpet1\",\n",
    "        \"mallar3\",\n",
    "        \"masboo\",\n",
    "        \"mauala\",\n",
    "        \"maupar\",\n",
    "        \"merlin\",\n",
    "        \"mitpar\",\n",
    "        \"moudov\",\n",
    "        \"norcar\",\n",
    "        \"norhar2\",\n",
    "        \"normoc\",\n",
    "        \"norpin\",\n",
    "        \"norsho\",\n",
    "        \"nutman\",\n",
    "        \"oahama\",\n",
    "        \"omao\",\n",
    "        \"osprey\",\n",
    "        \"pagplo\",\n",
    "        \"palila\",\n",
    "        \"parjae\",\n",
    "        \"pecsan\",\n",
    "        \"peflov\",\n",
    "        \"perfal\",\n",
    "        \"pibgre\",\n",
    "        \"pomjae\",\n",
    "        \"puaioh\",\n",
    "        \"reccar\",\n",
    "        \"redava\",\n",
    "        \"redjun\",\n",
    "        \"redpha1\",\n",
    "        \"refboo\",\n",
    "        \"rempar\",\n",
    "        \"rettro\",\n",
    "        \"ribgul\",\n",
    "        \"rinduc\",\n",
    "        \"rinphe\",\n",
    "        \"rocpig\",\n",
    "        \"rorpar\",\n",
    "        \"rudtur\",\n",
    "        \"ruff\",\n",
    "        \"saffin\",\n",
    "        \"sander\",\n",
    "        \"semplo\",\n",
    "        \"sheowl\",\n",
    "        \"shtsan\",\n",
    "        \"skylar\",\n",
    "        \"snogoo\",\n",
    "        \"sooshe\",\n",
    "        \"sooter1\",\n",
    "        \"sopsku1\",\n",
    "        \"sora\",\n",
    "        \"spodov\",\n",
    "        \"sposan\",\n",
    "        \"towsol\",\n",
    "        \"wantat1\",\n",
    "        \"warwhe1\",\n",
    "        \"wesmea\",\n",
    "        \"wessan\",\n",
    "        \"wetshe\",\n",
    "        \"whfibi\",\n",
    "        \"whiter\",\n",
    "        \"whttro\",\n",
    "        \"wiltur\",\n",
    "        \"yebcar\",\n",
    "        \"yefcan\",\n",
    "        \"zebdov\",\n",
    "        ]\n",
    "    bird2id = {b:i for i,b in enumerate(target_columns)}\n",
    "    id2bird = {i:b for i,b in enumerate(target_columns)}\n",
    "    scored_birds = [\"akiapo\", \"aniani\", \"apapan\", \"barpet\", \"crehon\", \"elepai\", \"ercfra\", \"hawama\", \"hawcre\", \"hawgoo\", \"hawhaw\", \"hawpet1\", \"houfin\", \"iiwi\", \"jabwar\", \"maupar\", \"omao\", \"puaioh\", \"skylar\", \"warwhe1\", \"yefcan\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeMelSpec(nn.Module):\n",
    "    def __init__(self,eps = 1e-12):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self,x):\n",
    "\n",
    "        mean = x.mean((1,2),keepdim = True)\n",
    "        std = x.std((1,2),keepdim = True)\n",
    "        x_std = (x-mean)/(std+self.eps)\n",
    "\n",
    "        norm_min = x_std.min(-1)[0].min(-1)[0]\n",
    "        norm_max = x_std.max(-1)[0].max(-1)[0]\n",
    "\n",
    "        fix_ind = (norm_max - norm_min) > self.eps\n",
    "        fix_ind = (fix_ind * torch.ones_like((norm_max - norm_min))).long()\n",
    "        \n",
    "        v = torch.zeros_like(x_std)\n",
    "        \n",
    "        #归一化后存在非零特征值(保留下来的是对应的batch)\n",
    "        if fix_ind.sum():\n",
    "            v_fix = x_std[fix_ind]\n",
    "            norm_max_fix = norm_max[fix_ind,None,None]\n",
    "            norm_min_fix = norm_min[fix_ind,None,None]\n",
    "            v_fix = torch.max(\n",
    "                torch.min(v_fix,norm_max_fix),\n",
    "                norm_min_fix,\n",
    "            )\n",
    "            v_fix = (v_fix - norm_min_fix)/(norm_max_fix - norm_min_fix)\n",
    "            v[fix_ind] = v_fix\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "        \n",
    "class ASTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The AST model.\n",
    "    :param label_dim: the label dimension, i.e., the number of total classes, it is 527 for AudioSet, 50 for ESC-50, and 35 for speechcommands v2-35\n",
    "    :param fstride: the stride of patch spliting on the frequency dimension, for 16*16 patchs, fstride=16 means no overlap, fstride=10 means overlap of 6\n",
    "    :param tstride: the stride of patch spliting on the time dimension, for 16*16 patchs, tstride=16 means no overlap, tstride=10 means overlap of 6\n",
    "    :param input_fdim: the number of frequency bins of the input spectrogram\n",
    "    :param input_tdim: the number of time frames of the input spectrogram\n",
    "    :param imagenet_pretrain: if use ImageNet pretrained model\n",
    "    :param audioset_pretrain: if use full AudioSet and ImageNet pretrained model\n",
    "    :param model_size: the model size of AST, should be in [tiny224, small224, base224, base384], base224 and base 384 are same model, but are trained differently during ImageNet pretraining.\n",
    "    \"\"\"\n",
    "    def __init__(self, label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=False,audioset_pretrain=False, model_size='tiny224', verbose=True):\n",
    "\n",
    "        super(ASTModel, self).__init__()\n",
    "        #assert timm.__version__ == '0.4.5', 'Please use timm == 0.4.5, the code might not be compatible with newer versions.'\n",
    "\n",
    "        if verbose == True:\n",
    "            print('---------------AST Model Summary---------------')\n",
    "            print('ImageNet pretraining: {:s}, AudioSet pretraining: {:s}'.format(str(imagenet_pretrain),str(audioset_pretrain)))\n",
    "        # override timm input shape restriction\n",
    "        timm.models.vision_transformer.PatchEmbed = PatchEmbed\n",
    "\n",
    "        # if AudioSet pretraining is not used (but ImageNet pretraining may still apply)\n",
    "        if audioset_pretrain == False:\n",
    "            if model_size == 'tiny224':\n",
    "                self.v = timm.create_model('vit_deit_tiny_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
    "                #self.v = timm.create_model('vit_tiny_patch16_224', pretrained=imagenet_pretrain)\n",
    "            elif model_size == 'small224':\n",
    "                self.v = timm.create_model('vit_deit_small_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
    "                #self.v = timm.create_model('vit_small_patch16_224', pretrained=imagenet_pretrain)\n",
    "            elif model_size == 'base224':\n",
    "                self.v = timm.create_model('vit_deit_base_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
    "                #self.v = timm.create_model('vit_base_patch16_224', pretrained=imagenet_pretrain)\n",
    "            elif model_size == 'base384':\n",
    "                self.v = timm.create_model('vit_deit_base_distilled_patch16_384', pretrained=imagenet_pretrain)\n",
    "                #self.v = timm.create_model('vit_base_patch16_384', pretrained=imagenet_pretrain)\n",
    "            else:\n",
    "                raise Exception('Model size must be one of tiny224, small224, base224, base384.')\n",
    "            self.original_num_patches = self.v.patch_embed.num_patches\n",
    "            self.oringal_hw = int(self.original_num_patches ** 0.5)\n",
    "            self.original_embedding_dim = self.v.pos_embed.shape[2]\n",
    "            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n",
    "\n",
    "            # automatcially get the intermediate shape\n",
    "            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n",
    "            num_patches = f_dim * t_dim\n",
    "            self.v.patch_embed.num_patches = num_patches\n",
    "            if verbose == True:\n",
    "                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n",
    "                print('number of patches={:d}'.format(num_patches))\n",
    "\n",
    "            # the linear projection layer\n",
    "            new_proj = torch.nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
    "            if imagenet_pretrain == True:\n",
    "                new_proj.weight = torch.nn.Parameter(torch.sum(self.v.patch_embed.proj.weight, dim=1).unsqueeze(1))\n",
    "                new_proj.bias = self.v.patch_embed.proj.bias\n",
    "            self.v.patch_embed.proj = new_proj\n",
    "\n",
    "            # the positional embedding\n",
    "            if imagenet_pretrain == True:\n",
    "                # get the positional embedding from deit model, skip the first two tokens (cls token and distillation token), reshape it to original 2D shape (24*24).\n",
    "                new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, self.original_num_patches, self.original_embedding_dim).transpose(1, 2).reshape(1, self.original_embedding_dim, self.oringal_hw, self.oringal_hw)\n",
    "                # cut (from middle) or interpolate the second dimension of the positional embedding\n",
    "                if t_dim <= self.oringal_hw:\n",
    "                    new_pos_embed = new_pos_embed[:, :, :, int(self.oringal_hw / 2) - int(t_dim / 2): int(self.oringal_hw / 2) - int(t_dim / 2) + t_dim]\n",
    "                else:\n",
    "                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(self.oringal_hw, t_dim), mode='bilinear')\n",
    "                # cut (from middle) or interpolate the first dimension of the positional embedding\n",
    "                if f_dim <= self.oringal_hw:\n",
    "                    new_pos_embed = new_pos_embed[:, :, int(self.oringal_hw / 2) - int(f_dim / 2): int(self.oringal_hw / 2) - int(f_dim / 2) + f_dim, :]\n",
    "                else:\n",
    "                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n",
    "                # flatten the positional embedding\n",
    "                new_pos_embed = new_pos_embed.reshape(1, self.original_embedding_dim, num_patches).transpose(1,2)\n",
    "                # concatenate the above positional embedding with the cls token and distillation token of the deit model.\n",
    "                self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n",
    "            else:\n",
    "                # if not use imagenet pretrained model, just randomly initialize a learnable positional embedding\n",
    "                # TODO can use sinusoidal positional embedding instead\n",
    "                new_pos_embed = nn.Parameter(torch.zeros(1, self.v.patch_embed.num_patches + 2, self.original_embedding_dim))\n",
    "                self.v.pos_embed = new_pos_embed\n",
    "                trunc_normal_(self.v.pos_embed, std=.02)\n",
    "\n",
    "        # now load a model that is pretrained on both ImageNet and AudioSet\n",
    "        elif audioset_pretrain == True:\n",
    "            if audioset_pretrain == True and imagenet_pretrain == False:\n",
    "                raise ValueError('currently model pretrained on only audioset is not supported, please set imagenet_pretrain = True to use audioset pretrained model.')\n",
    "            if model_size != 'base384':\n",
    "                raise ValueError('currently only has base384 AudioSet pretrained model.')\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            if os.path.exists('../pretrained_models/audioset_10_10_0.4593.pth') == False:\n",
    "                # this model performs 0.4593 mAP on the audioset eval set\n",
    "                audioset_mdl_url = 'https://www.dropbox.com/s/cv4knew8mvbrnvq/audioset_0.4593.pth?dl=1'\n",
    "                wget.download(audioset_mdl_url, out='../pretrained_models/audioset_10_10_0.4593.pth')\n",
    "            sd = torch.load('../pretrained_models/audioset_10_10_0.4593.pth', map_location=device)\n",
    "            audio_model = ASTModel(label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=False, audioset_pretrain=False, model_size='base384', verbose=False)\n",
    "            audio_model = torch.nn.DataParallel(audio_model)\n",
    "            audio_model.load_state_dict(sd, strict=False)\n",
    "            self.v = audio_model.module.v\n",
    "            self.original_embedding_dim = self.v.pos_embed.shape[2]\n",
    "            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n",
    "\n",
    "            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n",
    "            num_patches = f_dim * t_dim\n",
    "            self.v.patch_embed.num_patches = num_patches\n",
    "            if verbose == True:\n",
    "                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n",
    "                print('number of patches={:d}'.format(num_patches))\n",
    "\n",
    "            new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, 1212, 768).transpose(1, 2).reshape(1, 768, 12, 101)\n",
    "            # if the input sequence length is larger than the original audioset (10s), then cut the positional embedding\n",
    "            if t_dim < 101:\n",
    "                new_pos_embed = new_pos_embed[:, :, :, 50 - int(t_dim/2): 50 - int(t_dim/2) + t_dim]\n",
    "            # otherwise interpolate\n",
    "            else:\n",
    "                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(12, t_dim), mode='bilinear')\n",
    "            new_pos_embed = new_pos_embed.reshape(1, 768, num_patches).transpose(1, 2)\n",
    "            self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n",
    "\n",
    "    def get_shape(self, fstride, tstride, input_fdim=128, input_tdim=1024):\n",
    "        test_input = torch.randn(1, 1, input_fdim, input_tdim)\n",
    "        test_proj = nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
    "        test_out = test_proj(test_input)\n",
    "        f_dim = test_out.shape[2]\n",
    "        t_dim = test_out.shape[3]\n",
    "        return f_dim, t_dim\n",
    "\n",
    "    #@autocast()\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: the input spectrogram, expected shape: (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        :return: prediction\n",
    "        \"\"\"\n",
    "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        B = x.shape[0]\n",
    "        x = self.v.patch_embed(x)\n",
    "        cls_tokens = self.v.cls_token.expand(B, -1, -1)\n",
    "        dist_token = self.v.dist_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
    "        x = x + self.v.pos_embed\n",
    "        x = self.v.pos_drop(x)\n",
    "        for blk in self.v.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.v.norm(x)\n",
    "        x = (x[:, 0] + x[:, 1]) / 2\n",
    "\n",
    "        x = self.mlp_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model used for prediction\n",
    "\n",
    "#GeM pooling\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'\n",
    "#model\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, model_name, embedding_size, pretrained=True):\n",
    "        super(BirdCLEFModel, self).__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        in_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Identity()\n",
    "        self.model.global_pool = nn.Identity()\n",
    "        self.pooling = GeM()\n",
    "        self.embedding = nn.Linear(in_features, embedding_size)\n",
    "        self.fc = nn.Linear(embedding_size, CONFIG.num_class)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.model(images)\n",
    "        pooled_features = self.pooling(features).flatten(1)\n",
    "        embedding = self.embedding(pooled_features)\n",
    "        output = self.fc(embedding)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model wraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMP_model():\n",
    "    def __init__(self,model_name):\n",
    "        if model_name == 'AST':\n",
    "            self.model = ASTModel(label_dim = CONFIG.num_class,input_fdim = CONFIG.n_mels,input_tdim = 313)\n",
    "        elif model_name == 'tf_efficientnet_b0_ns':\n",
    "            self.model = BirdCLEFModel(model_name,CONFIG.embedding_size)\n",
    "        self.model = self.model.to(CONFIG.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset for test\n",
    "class test_dataset(Dataset):\n",
    "\n",
    "    def __init__(self,df,clip,target_sample_rate = 32000):\n",
    "        self.df = df\n",
    "        self.clip = torch.mean(clip,axis = 0)\n",
    "        self.SR = target_sample_rate\n",
    "        self.num_samples = CONFIG.max_time*self.SR\n",
    "        self.normalizer = NormalizeMelSpec()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        sample = self.df.loc[idx, :]\n",
    "        row_id = sample.row_id\n",
    "\n",
    "        end = int(sample.seconds)\n",
    "        start = int(end - 5)\n",
    "\n",
    "        start_index = int(self.SR*start)\n",
    "        end_index = int(self.SR*end)\n",
    "\n",
    "        sample = self.clip[start_index:end_index]\n",
    "        \n",
    "        if sample.shape[0] > self.num_samples:\n",
    "            sample = self.crop_audio(sample)\n",
    "        if sample.shape[0] < self.num_samples:\n",
    "            sample = self.pad_audio(sample)\n",
    "        \n",
    "        sample = torch.nan_to_num(sample)\n",
    "        mel_spectrogram = MelSpectrogram(sample_rate=self.SR,\n",
    "                                        n_mels = CONFIG.n_mels,\n",
    "                                        n_fft = CONFIG.n_fft)\n",
    "        mel = mel_spectrogram(sample)\n",
    "        image = torch.stack([mel,mel,mel])\n",
    "        image = self.normalizer(image)\n",
    "        #image = torch.mean(self.normalizer(image),dim = 0).squeeze()\n",
    "        #image = image.permute(1,0)\n",
    "        #max_val = torch.abs(image).max()\n",
    "        #image = image / max_val\n",
    "        return image,row_id,end\n",
    "    \n",
    "\n",
    "    def pad_audio(self, audio):\n",
    "        pad_length = self.num_samples - audio.shape[0]\n",
    "        last_dim_padding = (0, pad_length)\n",
    "        audio = F.pad(audio, last_dim_padding) #奇怪的pad方式增加了\n",
    "        return audio\n",
    "        \n",
    "    def crop_audio(self, audio):\n",
    "        return audio[:self.num_samples] \n",
    "\n",
    "# \n",
    "def prediction_for_clip(test_df,clip,models):\n",
    "    dataset = test_dataset(df = test_df,clip = clip)\n",
    "    loader = DataLoader(dataset,batch_size = 1,shuffle = False)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    prediction_dict = {'row_id':[],'target':[]}\n",
    "    for image , row_id,seconds in tqdm(loader):\n",
    "        image = image.to(device)\n",
    "        pred_total = torch.zeros(CONFIG.num_class).to(device)\n",
    "        for i in range(len(models)):\n",
    "            if CONFIG.model_name[i] == 'AST':\n",
    "                input = (torch.mean(image.squeeze(),dim = 0)).permute(1,0).unsqueeze(dim = 0)\n",
    "            else:\n",
    "                input = image\n",
    "            outputs = models[i](input)\n",
    "            pred = torch.sigmoid(outputs)[0]\n",
    "            pred_total += pred\n",
    "        pred_total = pred/len(models)\n",
    "        row_id = row_id[0]\n",
    "        seconds = seconds.item()\n",
    "        for bird in CONFIG.scored_birds:\n",
    "            judge = False\n",
    "            if pred_total[int(CONFIG.bird2id[bird])] >= 0.2:\n",
    "                judge = True\n",
    "            id = row_id + '_' + bird + '_'+str(seconds)\n",
    "                \n",
    "            prediction_dict['row_id'].append(id)\n",
    "            prediction_dict['target'].append(judge)\n",
    "\n",
    "    return prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction part\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "\n",
    "def prepare_model_for_inference(model,path,device = 'cuda:1'):\n",
    "    if not torch.cuda.is_available():\n",
    "        ckpt = torch.load(path,map_location = 'cpu')\n",
    "    else:\n",
    "        ckpt = torch.load(path,map_location={'cuda:1' :'cuda:0'})\n",
    "    model.load_state_dict(ckpt)\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "def prediction(test_audios,models,threshold = 0.05, threshold_long = None):\n",
    "    #假设这里的model已经完成了load\n",
    "    prediction_dicts = {'row_id':[],'target':[]}\n",
    "    for audio_path in test_audios:\n",
    "        clip,_ = torchaudio.load(audio_path)\n",
    "        seconds = []\n",
    "        row_ids = []\n",
    "\n",
    "        for second in range(5,65,5):\n",
    "            row_id = audio_path.name.split('.')[:-1][0]\n",
    "            #row_id = \"_\".join(audio_path.name.split('.'[:-1])+f\"_{second}\")\n",
    "            seconds.append(second)\n",
    "            row_ids.append(row_id)\n",
    "        \n",
    "        test_df = pd.DataFrame(\n",
    "            {\n",
    "                \"row_id\":row_ids,\n",
    "                \"seconds\":seconds\n",
    "            }\n",
    "        )\n",
    "        prediction_dict = prediction_for_clip(test_df,clip,models)\n",
    "        prediction_dicts['row_id'].extend(prediction_dict['row_id'])\n",
    "        prediction_dicts['target'].extend(prediction_dict['target'])\n",
    "    \n",
    "    return prediction_dicts\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: False, AudioSet pretraining: False\n",
      "frequncey stride=10, time stride=10\n",
      "number of patches=630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:03<00:00,  3.40it/s]\n",
      "100%|██████████| 12/12 [00:01<00:00,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          row_id  target\n",
      "0  soundscape_453028782_akiapo_5   False\n",
      "1  soundscape_453028782_aniani_5   False\n",
      "2  soundscape_453028782_apapan_5   False\n",
      "3  soundscape_453028782_barpet_5   False\n",
      "4  soundscape_453028782_crehon_5   False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_audio_dir = Path('input/birdcleff-2022/test_soundscapes/')\n",
    "torch.cuda.empty_cache()\n",
    "#test_audio_dir = Path('input/birdcleff-2022/test_soundscapes/')\n",
    "all_audios = list(test_audio_dir.glob(\"*.ogg\"))\n",
    "\n",
    "model_list = []\n",
    "trained_list = ['AST_F10.4501_epoch182.bin','F10.4293_epoch120.bin']\n",
    "for i in range(len(trained_list)):\n",
    "    model = CMP_model(CONFIG.model_name[i]).model\n",
    "    torch.cuda.empty_cache()\n",
    "    model = prepare_model_for_inference(model,trained_list[i])\n",
    "    model_list.append(model)\n",
    "\n",
    "\n",
    "pred = prediction(all_audios,model_list)\n",
    "\n",
    "'''\n",
    "if CONFIG.model_name == 'AST':\n",
    "    model = ASTModel(label_dim = CONFIG.num_class, input_fdim = 224, input_tdim = 313)\n",
    "    model.to(CONFIG.device)\n",
    "\n",
    "model = prepare_model_for_inference(model,'F10.4327_epoch166.bin')\n",
    "pred = prediction(all_audios,model)\n",
    "'''\n",
    "result = pd.DataFrame(pred,columns = ['row_id','target'])\n",
    "print(result.head())\n",
    "result.to_csv(\"submission.csv\",index = False)\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c5a4c13e11645c90d5721dfc4f59fec06c6fa93fc9ef7e389b8f49441585888"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('sfn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
